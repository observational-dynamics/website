{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"lonestar108/base\" # The model that you want to train from the Hugging Face hub\n",
    "dataset_name = \"lonestar108/enlightenedllm\" # The instruction dataset to use\n",
    "new_model = \"enlightenedllm\" # Fine-tuned model name\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "lora_r = 64 # LoRA attention dimension\n",
    "lora_alpha = 16 # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1 # Dropout probability for LoRA layers\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "use_4bit = True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "output_dir = \"./results\" # Output directory where the model predictions and checkpoints will be store\n",
    "num_train_epochs = 5 # Number of training epochs\n",
    "fp16 = False # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "bf16 = True\n",
    "\n",
    "per_device_train_batch_size = 4 # Batch size per GPU for training\n",
    "per_device_eval_batch_size = 4 # Batch size per GPU for evaluation\n",
    "gradient_accumulation_steps = 1 # Number of update steps to accumulate the gradients for\n",
    "gradient_checkpointing = True # Enable gradient checkpointing\n",
    "max_grad_norm = 0.3 # Maximum gradient normal (gradient clipping)\n",
    "learning_rate = 2e-4 # Initial learning rate (AdamW optimizer)\n",
    "weight_decay = 0.001 # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "optim = \"paged_adamw_32bit\" # Optimizer to use\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "max_steps = -1 # Number of training steps (overrides num_train_epochs)\n",
    "warmup_ratio = 0.03 # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True # Group sequences into batches with same length\n",
    "save_steps = 0 # Save checkpoint every X updates steps\n",
    "logging_steps = 25 # Log every X updates steps\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length = None # Maximum sequence length to use\n",
    "packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "device_map = {\"\": 0} # Load the entire model on the GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"lonestar108/base\" # The model that you want to train from the Hugging Face hub\n",
    "dataset_name = \"lonestar108/enlightenedllm\" # The instruction dataset to use\n",
    "new_model = \"enlightenedllm\" # Fine-tuned model name\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "lora_r = 64 # LoRA attention dimension\n",
    "lora_alpha = 16 # Alpha parameter for LoRA scaling\n",
    "lora_dropout = 0.1 # Dropout probability for LoRA layers\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "use_4bit = True # Activate 4-bit precision base model loading\n",
    "bnb_4bit_compute_dtype = \"float16\" # Compute dtype for 4-bit base models\n",
    "bnb_4bit_quant_type = \"nf4\" # Quantization type (fp4 or nf4)\n",
    "use_nested_quant = False # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "output_dir = \"./results\" # Output directory where the model predictions and checkpoints will be store\n",
    "num_train_epochs = 5 # Number of training epochs\n",
    "fp16 = False # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "bf16 = True\n",
    "\n",
    "per_device_train_batch_size = 4 # Batch size per GPU for training\n",
    "per_device_eval_batch_size = 4 # Batch size per GPU for evaluation\n",
    "gradient_accumulation_steps = 1 # Number of update steps to accumulate the gradients for\n",
    "gradient_checkpointing = True # Enable gradient checkpointing\n",
    "max_grad_norm = 0.3 # Maximum gradient normal (gradient clipping)\n",
    "learning_rate = 2e-4 # Initial learning rate (AdamW optimizer)\n",
    "weight_decay = 0.001 # Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "optim = \"paged_adamw_32bit\" # Optimizer to use\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "max_steps = -1 # Number of training steps (overrides num_train_epochs)\n",
    "warmup_ratio = 0.03 # Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True # Group sequences into batches with same length\n",
    "save_steps = 0 # Save checkpoint every X updates steps\n",
    "logging_steps = 25 # Log every X updates steps\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length = None # Maximum sequence length to use\n",
    "packing = False # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "device_map = {\"\": 0} # Load the entire model on the GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'meta-llama/Llama-2-7b'\n",
    "model_save_paths = ['model_1.pt', 'model_2.pt', 'model_3.pt']\n",
    "random_seeds = [42, 123, 456]\n",
    "system_prompt = \"You are a spiritual teacher.\"\n",
    "\n",
    "# Loop to train the models\n",
    "for i in range(3):\n",
    "    load_and_train_model_with_dataset(model_name, dataset_name, system_prompt, model_save_paths[i], num_train_epochs, random_seeds[i])\n",
    "\n",
    "# Ensembling predictions\n",
    "models = []\n",
    "for path in model_save_paths:\n",
    "    model = AutoModelForCausalLM.from_pretrained(path)\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    models.append(model)\n",
    "\n",
    "dataset_test = load_dataset('lonestar108/enlightenedllm', split='test')  # specify 'test' or 'validation' for test set\n",
    "outputs = []\n",
    "for model in models:\n",
    "    # make prediction for each model\n",
    "    outputs.append(model(dataset_test))\n",
    "    \n",
    "# Averaging predictions\n",
    "ensemble_output = sum(outputs) / 3\n",
    "\n",
    "# Computing accuracy\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(len(ensemble_output)):\n",
    "    # get the predicted token\n",
    "    predicted_token = torch.argmax(ensemble_output[i][0][0][-1]).item()\n",
    "    # get the actual token\n",
    "    actual_token = dataset_test[i]['text'][-1]\n",
    "    # compare the predicted and actual tokens\n",
    "    if predicted_token == actual_token:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Print accuracy\n",
    "print(correct_predictions / len(ensemble_output))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
